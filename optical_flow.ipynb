{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2867,
     "status": "ok",
     "timestamp": 1701097026277,
     "user": {
      "displayName": "­문희준 | 인공지능학과 | 한양대(서울)",
      "userId": "12627775913057364184"
     },
     "user_tz": -540
    },
    "id": "yAKXA7Z7Gwz0",
    "outputId": "612196d8-1366-4650-c628-3ad957a2bf19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# connect google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IAMDrBMnClu"
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lrjtVs-oGODv"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHVW6Ht3nE_L"
   },
   "source": [
    "# Part1: goodFeaturesToTrack\n",
    "- Fill the missing part (denoted as ```fill here```) of the code\n",
    "- We provide procedure comments for complete the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AEPUBxp_GPpl"
   },
   "outputs": [],
   "source": [
    "def goodFeaturesToTrack(image, maxCorners=100, qualityLevel=0.03, blocksize=7):\n",
    "\n",
    "    # Image bluring wih averaging filter\n",
    "    # Only cv2.filter2D is allowed for convolution operation!\n",
    "    #노이즈를 줄이고 image를 부드럽게 만드는 과정 이를 통해 corner detection을 더 잘 되게 만듬\n",
    "    image = cv2.filter2D(image, -1, np.ones((blocksize, blocksize), np.float32) / (blocksize * blocksize))\n",
    "\n",
    "    # Compute gradients\n",
    "    Ix = cv2.filter2D(image, -1, np.array([[-1, 0, 1],\n",
    "                                           [-2, 0, 2],\n",
    "                                           [-1, 0, 1]], dtype=np.float32)) * (1 / 8)\n",
    "    Iy = cv2.filter2D(image, -1, np.array([[-1, -2, -1],\n",
    "                                           [0, 0, 0],\n",
    "                                           [1, 2, 1]], dtype=np.float32)) * (1 / 8)\n",
    "\n",
    "    # Compute products of gradients at each pixel\n",
    "    Ixx = Ix * Ix\n",
    "    Iyy = Iy * Iy\n",
    "    Ixy = Ix * Iy\n",
    "\n",
    "    # Compute the sums of products of gradients in local windows\n",
    "    Sxx = cv2.filter2D(Ixx, -1, np.ones((blocksize, blocksize)))\n",
    "    Syy = cv2.filter2D(Iyy, -1, np.ones((blocksize, blocksize)))\n",
    "    Sxy = cv2.filter2D(Ixy, -1, np.ones((blocksize, blocksize)))\n",
    "\n",
    "    # Compute the determinant and trace of the matrix M for each pixel\n",
    "    detM = (Sxx * Syy) - (Sxy * Sxy)\n",
    "    traceM = Sxx + Syy\n",
    "\n",
    "    # Compute the Harris response with detM and traceM\n",
    "    k = 0.04  # Harris detector free parameter\n",
    "    harris_response = detM - k * (traceM ** 2)\n",
    "\n",
    "    # Threshold the Harris response to find candidate corners\n",
    "    corners = np.argwhere(harris_response > (qualityLevel * harris_response.max()))\n",
    "\n",
    "    # Sort the corners by Harris response in descending order\n",
    "    sorted_corners = corners[np.argsort(harris_response[corners[:, 0], corners[:, 1]])[::-1]]\n",
    "\n",
    "    # Keep the top 'maxCorners' corners\n",
    "    selected_corners = sorted_corners[:maxCorners]\n",
    "\n",
    "    final_corners = np.array(selected_corners)\n",
    "    final_corners = final_corners.reshape(-1, 1, 2)\n",
    "\n",
    "    return final_corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gcb42xzntML"
   },
   "source": [
    "# Part2: Optical flow with Lukas-Kanade\n",
    "- Fill the missing part (denoted as ```fill here```) of the code\n",
    "- We provide procedure comments for complete the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lv7zvb_nG8ql"
   },
   "outputs": [],
   "source": [
    "\n",
    "def optical_flow(old_frame, new_frame, window_size, min_quality):\n",
    "\n",
    "    feature_list = goodFeaturesToTrack(old_frame, max_corners, min_quality, blocksize)\n",
    "\n",
    "    w = int(window_size/2)\n",
    "\n",
    "    # Normalize\n",
    "    old_frame = old_frame / 255\n",
    "    new_frame = new_frame / 255\n",
    "\n",
    "    # Convolve to get gradients w.r.to X, Y and T dimensions\n",
    "    kernel_x = np.array([[-1, 0, 1],\n",
    "                         [-2, 0, 2],\n",
    "                         [-1, 0, 1],], dtype=np.float32) * (1/8)\n",
    "    kernel_y = np.array([[-1, -2,-1],\n",
    "                         [0, 0, 0],\n",
    "                         [1, 2, 1]], dtype=np.float32) * (1/8)\n",
    "    #아래는 time gradients\n",
    "    kernel_t = np.array([[1, 1, 1],\n",
    "                         [1, 1, 1],\n",
    "                         [1, 1, 1]], dtype=np.float32) * (1/9)\n",
    "\n",
    "    # cv2.filter2D is allowed for convolution!\n",
    "    fx =  (cv2.filter2D(old_frame, -1, kernel_x) + cv2.filter2D(new_frame, -1, kernel_x)) \n",
    "    fy =  (cv2.filter2D(old_frame, -1, kernel_y) + cv2.filter2D(new_frame, -1, kernel_y))\n",
    "    #2개의 프레임차이를 계산하여 변화를 계산하는 것\n",
    "    ft =  (cv2.filter2D(new_frame, -1, kernel_t) + cv2.filter2D(old_frame, -1, kernel_t))\n",
    "\n",
    "    u = np.zeros(old_frame.shape)\n",
    "    v = np.zeros(old_frame.shape)\n",
    "\n",
    "    for feature in feature_list:  # for every corner\n",
    "        i, j = feature.ravel()  # get cordinates of the corners (i,j).\n",
    "        i, j = int(i), int(j)  # i,j are floats initially so convert to integer type\n",
    "\n",
    "        #윈도우 프레임 중심좌표에서 차이만큼 보는거 \n",
    "        I_x = fx[i - w: i + w + 1, j - w: j + w + 1].flatten()\n",
    "        I_y = fy[i - w: i + w + 1, j - w: j + w + 1].flatten()\n",
    "        I_t = ft[i - w: i + w + 1, j - w: j + w + 1].flatten()\n",
    "\n",
    "        b = np.reshape(I_t, (I_t.shape[0], 1))\n",
    "        A = np.vstack((I_x, I_y)).T\n",
    "\n",
    "        U = np.matmul(np.linalg.pinv(A), b)  # Solving for (u,v) i.e., U => A * U = b 푸는거 따라서 U는 A 역행렬에 b를 곱해서 구할수 있음\n",
    "\n",
    "        u[i, j] = U[0][0]\n",
    "        v[i, j] = U[1][0]\n",
    "\n",
    "    return (u, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4N4xHKEHAjA"
   },
   "source": [
    "# Main function\n",
    "- If part1 and part2 were filled properly, the 'output.avi' will be generated!\n",
    "- For google colab, as cv2.imshow() is not provided, so please use cv2_imshow (google.colab.patches) instead  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APQHNcblG_Jp",
    "outputId": "1191d4ce-4a66-46ac-d50b-d2a54b97a83e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "2024-12-03 13:35:48.889 python[30346:25280872] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-12-03 13:35:48.889 python[30346:25280872] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('/Users/parkjunseo/Desktop/Hanyang/4학년 2학기/컴퓨터비전개론/assignment/ICPBL/slow.mp4')\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "\n",
    "# Width and height of the file to save\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "# 'output.mp4' will be generated!\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "out = cv2.VideoWriter('output.mp4',  fourcc, 30.0, (int(width), int(height)))\n",
    "\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Shi Tomasi parameter\n",
    "max_corners = 100\n",
    "min_quality = 0.3\n",
    "blocksize = 7\n",
    "p0 = goodFeaturesToTrack(old_frame, max_corners, min_quality, blocksize)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while(1):\n",
    "    ret, current_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calculate optical flow\n",
    "    U, V = optical_flow(old_gray, frame_gray, 15, 0.03)\n",
    "    for i in range(current_frame.shape[0]):\n",
    "        for j in range(current_frame.shape[1]):\n",
    "            u, v = U[i][j], V[i][j]\n",
    "            if u and v:\n",
    "                mask = cv2.line(mask, (j, i), (int(round(j + u)), int(round(i + v))), (0, 255, 0), 2)\n",
    "                frame = cv2.arrowedLine(current_frame, (j, i), (int(round(j + u)), int(round(i + v))), (0, 255, 0), thickness=2)\n",
    "                current_frame = cv2.add(current_frame, mask)\n",
    "\n",
    "    # Display the frame with optical flow vectors\n",
    "    cv2.imshow('optical flow' ,current_frame)\n",
    "    out.write(current_frame)\n",
    "    # Break the loop if 'Esc' key is pressed\n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "\n",
    "    # Set the current frame as the previous frame for the next iteration\n",
    "    old_gray = frame_gray\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close the plot window when done\n",
    "plt.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "icpbl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
